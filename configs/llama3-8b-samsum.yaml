# Model configuration
model:
  name: "meta-llama/Llama-2-7b-hf"
  max_length: 2048
  use_flash_attention: true
  hidden_size: 4096

# Training configuration
training:
  learning_rate: 2e-4
  weight_decay: 0.01
  num_epochs: 3
  warmup_steps: 100
  batch_size: 8
  gradient_accumulation_steps: 4
  eval_steps: 100
  save_steps: 500
  use_fp16: true
  max_steps: 1000
  mixed_precision: "bf16"
  gradient_checkpointing: true

# Dataset configuration
dataset:
  name: "samsum"
  train_split: "train"
  val_split: "validation"
  test_split: "test"
  max_samples: 10000
  validation_split: 0.1
  preprocessing:
    max_length: 2048
    truncation: true
    padding: "max_length"

# PolyAdapter configuration
polyadapter:
  use_lora: true
  use_ia3: true
  use_moe: true
  rank: 8
  num_experts: 4
  k: 1
  dropout: 0.1

# AutoRank configuration
autorank:
  metric: "loss"
  mode: "min"
  rank_budget: 1024
  rank_candidates: [0, 2, 4, 8, 16]
  num_trials: 30
  patience: 5
  optimize_steps: 1000

# Progressive Freezing configuration
freezing_scheduler:
  threshold: 1e-5
  window_size: 10
  unfreeze_threshold: 0.1
  min_steps: 100
  min_layers: 4

# Memory configuration (optional)
use_memory: false
memory:
  memory_size: 1000
  feature_dim: 4096
  num_neighbors: 5
  temperature: 0.1

# Generation configuration
generation:
  max_length: 512
  num_beams: 4
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.2

# Evaluation configuration
evaluation:
  metrics:
    - "rouge"
    - "bleu"
  batch_size: 8
  use_fp16: true

# Logging configuration
logging:
  run_name: "llama3-8b-samsum"
  project: "athena"
  tags:
    - "llama3"
    - "samsum"
    - "summarization"
  wandb: true
  log_steps: 10
  eval_steps: 100
  save_steps: 500 